{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"_OtmK2N-wvbk","executionInfo":{"status":"ok","timestamp":1665394081304,"user_tz":-60,"elapsed":964,"user":{"displayName":"Joseph Amess","userId":"01893786635442142975"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn.datasets as dt"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"OTkQc1UMwvbm","executionInfo":{"status":"ok","timestamp":1665394082131,"user_tz":-60,"elapsed":173,"user":{"displayName":"Joseph Amess","userId":"01893786635442142975"}}},"outputs":[],"source":["class Utilities():\n","\n","    def entropy_calc(self, data):\n","\n","        # returns entropy scaler value #\n","        # calculates the entropy of target var in a dataset #\n","\n","        entropy = 0\n","        for target_val in np.unique(data[:, -1]):\n","            prob = len(data[data[:, -1] == target_val]) / len(data[:, -1])\n","            log_prob = np.log2(prob)\n","            entropy -= (prob * log_prob)\n","\n","        return entropy, len(data[:, -1])\n","\n","\n","    def information_gain_calc(self, split_val, feature_data):\n","\n","        # returns the information gain #\n","        # calculates the information gain of a split in data #\n","        joined_data = np.concatenate(   (np.reshape(feature_data[0], (feature_data[0].shape[0], 1)), \n","                                        np.reshape(feature_data[1], (feature_data[0].shape[0], 1))), \n","                                        axis=1\n","                                    )\n","        original_entropy, _ = self.entropy_calc(joined_data)\n","\n","        upper_split_entropy, upper_len = self.entropy_calc(joined_data[joined_data[:, 0] >= split_val])\n","        lower_split_entropy, lower_len = self.entropy_calc(joined_data[joined_data[:, 0] < split_val])\n","        lower_dist, upper_dist = (lower_len / (upper_len + lower_len)), (upper_len / (upper_len + lower_len))\n","        distributed_entropy = (upper_dist * upper_split_entropy) + (lower_dist * lower_split_entropy)\n","\n","        return original_entropy - distributed_entropy\n","\n","\n","    def partitioner(self, data, num_partitions, feature_num):\n","\n","        # partitions continuous data using information gain #\n","        # returns best split in continuous data for decision tree as well as new split data #\n","        feature_data = (data[0][:, feature_num], data[1])\n","        test_splits = np.arange(min(feature_data[0]),\n","                                max(feature_data[0]),\n","                                num_partitions + 1\n","                                )[1:]\n","\n","        best_split = 0\n","        best_information_gain = 0\n","        for split_val in test_splits:\n","            information_gain = self.information_gain_calc(split_val, feature_data)\n","            if information_gain > best_information_gain:\n","                best_information_gain = information_gain\n","                best_split = split_val\n","\n","        joined_data = np.concatenate((data[0], np.reshape(data[1], (data[0].shape[0], 1))), axis=1)\n","        joined_upper_split = joined_data[joined_data[:, feature_num] >= best_split]\n","        joined_lower_split = joined_data[joined_data[:, feature_num] < best_split]\n","        upper_split_data = (joined_upper_split[:, 0:-1], joined_upper_split[:, -1])\n","        lower_split_data = (joined_lower_split[:, 0:-1], joined_lower_split[:, -1])\n","        \n","        return best_split, best_information_gain, upper_split_data, lower_split_data\n","\n","\n","    def bagger(bag_size, num_bags, num_features, data):\n","\n","        # uses bagging and subspace sampling to create mini datasets for each tree #\n","        # returns a dictionary of bags of data with information about features selected #\n","\n","        data_dict = {}\n","\n","        for i in range(num_bags):\n","            unique = False\n","            while unique == False:\n","                rand_features = np.random.randint(0, data[0][0].shape[1], num_features)\n","                if len(np.unique(rand_features)) == len(rand_features):\n","                    unique = True\n","            rand_sample = np.random.randint(0, data[0][0].shape[0], bag_size)\n","            cropped_data = data[0][0][rand_sample, :]\n","            cropped_data_2 = cropped_data[:, rand_features]\n","            X_data = data[0][0][rand_sample, :]\n","            X_data = X_data[:, rand_features]\n","            y_data = data[0][1][rand_sample]\n","            data_dict[f'bag_{i}'] = {}\n","            data_dict[f'bag_{i}']['data'] = (X_data, y_data)\n","            data_dict[f'bag_{i}']['samples'] = rand_sample\n","            data_dict[f'bag_{i}']['features'] = rand_features\n","\n","        return data_dict\n","\n","\n","    def build_node_data_storage(self, max_tree_depth):\n","\n","        # builds a tree to store data at each node #\n","        # returns a dictionary to store split data at nodes #\n","\n","        data_storage_tree = {}\n","        for i in range(max_tree_depth):\n","            data_storage_tree[f'level_{i}'] = {}\n","            for j in range(0, 2 ** i):\n","                data_storage_tree[f'level_{i}'][f'node_{j}'] = np.array([])\n","        \n","        return data_storage_tree\n","\n","    \n","    def train_test_val_split(split_vals, data):\n","\n","        # splits data into training, testing and validation sets #\n","        # returns split data #\n","\n","        np.random.seed(42)\n","        indices = np.arange(data[0].shape[0])\n","        np.random.shuffle(indices)\n","        train_data = (  data[0][indices[0:int(np.floor(indices.shape[0] * split_vals[0]))]],\n","                        data[1][indices[0:int(np.floor(indices.shape[0] * split_vals[0]))]]\n","                    )\n","        test_data = (   data[0][indices[int(np.ceil(indices.shape[0] * split_vals[0])): \n","                                        int(np.floor(indices.shape[0] * (split_vals[0] + split_vals[1])))]],\n","                        data[1][indices[int(np.ceil(indices.shape[0] * split_vals[0])): \n","                                        int(np.floor(indices.shape[0] * (split_vals[0] + split_vals[1])))]]\n","                    )\n","        val_data = (    data[0][indices[int(np.ceil(indices.shape[0] * (split_vals[0] + split_vals[1]))):]],\n","                        data[1][indices[int(np.ceil(indices.shape[0] * (split_vals[0] + split_vals[1]))):]]\n","                    )\n","        return (train_data, test_data, val_data)\n","    \n","\n","    def entropy_check(forest):\n","\n","        # checks drop in entropy down tree levels #\n","        # prints graph showing entropy drop in trees #\n","\n","        plt.figure(figsize=(15, 8))\n","        plt.title('Average entropy drop in tree layers within forest')\n","        plt.grid()\n","\n","        colours = ['r', 'g', 'b', 'c', 'm', 'k', 'y']\n","\n","        for tree in forest:\n","            level_count = 0\n","            entropy_list = []\n","            for level in forest[tree]:\n","                level_count += 1\n","                total_entropy = 0\n","                node_count = 0\n","                for node in forest[tree][level]:\n","                    if len(forest[tree][level][node]) != 0:\n","                        total_entropy += forest[tree][level][node]['entropy']\n","                        node_count += 1\n","                entropy_list.append(total_entropy / node_count)\n","            plt.plot(list(range(level_count)), entropy_list, c=colours[np.random.randint(0, len(colours))])\n","        \n","        plt.xlabel('Tree levels')\n","        plt.ylabel('Entropy')\n","        plt.show()\n","\n","\n","    def best_feature_check(forest):\n","\n","        # checks which features have the best information gain / most useful #\n","        # prints graph showing information gain of each feature #\n","\n","        pass\n","\n","\n","    def leaf_decision_choice(self, data):\n","\n","        # determines whether leaf node will predict 0 or 1 #\n","        # returns a 0 or 1 #\n","\n","        decision = 1\n","        if data[data[:, -1] == 0].shape[0] >= data[data[:, -1] == 1].shape[0]:\n","            decision = 0\n","        \n","        return decision\n","    \n","\n","    def accuracy(y, y_bar):\n","        \n","        # calculates accuracy #\n","        # returns accuracy of model on labels input #\n","\n","        difference = abs(y - y_bar)\n","        return 100 * (len(difference[difference == 0]) / len(y))\n","    \n","\n","    def grid_size_calc(param_dict):\n","\n","        # calculates number of tests to perform #\n","        # returns integer of number of tests to run #\n","        total_tests = 1\n","        for param in param_dict:\n","            total_tests *= len(param_dict[param])\n","        \n","        return total_tests\n","    \n","    def hyperparam_analysis(hyperparam_choices, param_dict, performance_dict):\n","\n","        # plot graph of two chosen hyperparameters against model accuracy #\n","        fig = plt.figure(figsize=(15, 15))\n","        ax = plt.axes(projection='3d')\n","        accuracy_array = np.zeros((len(param_dict[hyperparam_choices[0]]),\n","                                len(param_dict[hyperparam_choices[1]])))\n","        param_1_array = np.zeros((len(param_dict[hyperparam_choices[0]]),\n","                                len(param_dict[hyperparam_choices[1]])))\n","        param_2_array = np.zeros((len(param_dict[hyperparam_choices[0]]),\n","                                len(param_dict[hyperparam_choices[1]])))\n","        x_counter = 0\n","        y_counter = 0\n","        for test in performance_dict:\n","            accuracy_array[x_counter, y_counter] = performance_dict[test]['accuracy']\n","            param_1_array[x_counter, y_counter] = performance_dict[test][hyperparam_choices[0]]\n","            param_2_array[x_counter, y_counter] = performance_dict[test][hyperparam_choices[1]]\n","            x_counter += 1\n","            if x_counter == len(param_dict[hyperparam_choices[0]]):\n","                x_counter = 0\n","                y_counter += 1\n","        ax.plot_surface(param_1_array, \n","                        param_2_array,\n","                        accuracy_array, \n","                        rstride=1, \n","                        cstride=1, \n","                        cmap='viridis', \n","                        edgecolor='none')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QTAWt9Tmwvbq","executionInfo":{"status":"ok","timestamp":1665394087664,"user_tz":-60,"elapsed":178,"user":{"displayName":"Joseph Amess","userId":"01893786635442142975"}}},"outputs":[],"source":["class RandomForest(Utilities):\n","    def __init__(self, num_trees, max_tree_depth, min_entropy, num_partitions):\n","        super(RandomForest, self).__init__()\n","        self.num_trees = num_trees\n","        self.max_tree_depth = max_tree_depth\n","        self.min_entropy = min_entropy\n","        self.num_partitions = num_partitions\n","    \n","\n","    def build_forest(self):\n","        \n","        # builds trees ready for fitting #\n","        # returns a dictionary of trees ready for use #\n","\n","        forest = {}\n","        for i in range(self.num_trees):\n","            forest[f'tree_{i}'] = {}\n","            for j in range(self.max_tree_depth):\n","                forest[f'tree_{i}'][f'level_{j}'] = {}\n","                for k in range(0, 2 ** j):\n","                    forest[f'tree_{i}'][f'level_{j}'][f'node_{k}'] = {}\n","        \n","        return forest\n","    \n","\n","    def fit(self, bagged_data, forest):\n","\n","        # fits forest to training data #\n","        # returns fit forest #\n","\n","        for i, bag in enumerate(bagged_data):\n","            node_data_dict = self.build_node_data_storage(self.max_tree_depth)\n","            node_data_dict['level_0']['node_0'] = bagged_data[bag]['data']\n","            for j in range(self.max_tree_depth):\n","                for k in range(0, 2 ** j):\n","                    node_data = node_data_dict[f'level_{j}'][f'node_{k}']\n","                    # check if any data at node\n","                    if len(node_data) != 0:\n","                        joined_node_data = np.concatenate(( node_data[0], \n","                                                            np.reshape(node_data[1], (node_data[0].shape[0], 1))), \n","                                                            axis=1)\n","                        # if entropy at node is zero then store info but don't create new nodes #\n","                        if self.entropy_calc(joined_node_data)[0] == 0:\n","                            forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['type'] = 'leaf'\n","                            forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['decision'] = self.leaf_decision_choice(joined_node_data)\n","                            forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['entropy'] = self.entropy_calc(joined_node_data)[0]\n","                        \n","                        # if not at max depth and not 0 entropy then do create new nodes #\n","                        elif (self.entropy_calc(joined_node_data)[0] != 0) and ((j + 1) < self.max_tree_depth):\n","                            forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['type'] = 'node'\n","                            forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['entropy'] = self.entropy_calc(joined_node_data)[0]\n","                            best_information_gain = 0\n","                            for indice, feature_num in enumerate(bagged_data[bag]['features']):\n","                                split, information_gain, upper_split_data, lower_split_data = self.partitioner( node_data, \n","                                                                                                                self.num_partitions, \n","                                                                                                                indice\n","                                                                                                                )\n","                                if information_gain >= best_information_gain:\n","                                    best_information_gain = information_gain\n","                                    forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['feature'] = feature_num\n","                                    forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['split'] = split\n","                                    forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['info_gain'] = best_information_gain\n","                                    node_data_dict[f'level_{j + 1}'][f'node_{2 * k}'] = lower_split_data\n","                                    node_data_dict[f'level_{j + 1}'][f'node_{(2 * k) + 1}'] = upper_split_data\n","                        \n","                        # if at max depth then store info at node but don't create new nodes #\n","                        else:\n","                            forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['type'] = 'leaf'\n","                            forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['decision'] = self.leaf_decision_choice(joined_node_data)\n","                            forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['entropy'] = self.entropy_calc(joined_node_data)[0]\n","                            best_information_gain = 0\n","                            for indice, feature_num in enumerate(bagged_data[bag]['features']):\n","                                split, information_gain, upper_split_data, lower_split_data = self.partitioner( node_data, \n","                                                                                                                self.num_partitions, \n","                                                                                                                indice\n","                                                                                                                )\n","                                if information_gain >= best_information_gain:\n","                                    best_information_gain = information_gain\n","                                    forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['feature'] = feature_num\n","                                    forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['split'] = split\n","                                    forest[f'tree_{i}'][f'level_{j}'][f'node_{k}']['info_gain'] = best_information_gain\n","\n","        return forest\n","\n","\n","    def predict(self, test_data, forest):\n","\n","        # uses trained forest to make predictions on test data #\n","        # returns predictions #\n","\n","        predictions = np.zeros((test_data[0].shape[0]))\n","        for i in range(test_data[0].shape[0]):\n","            data_point = test_data[0][i, :]\n","            for tree in forest:\n","                current_tree = forest[tree]\n","                current_node_number = 0\n","                for j, level in enumerate(current_tree):\n","                    node = current_tree[level][f'node_{current_node_number}']\n","                    if node['type'] == 'node':\n","                        if data_point[node['feature']] >= node['split']:\n","                            current_node_number = (2 * current_node_number) + 1\n","                        else:\n","                            current_node_number = 2 * current_node_number\n","                    else:\n","                        predictions[i] = node['decision']\n","                        break\n","        \n","        return predictions"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I0j_7-5fwvbt","executionInfo":{"status":"ok","timestamp":1665395556298,"user_tz":-60,"elapsed":632072,"user":{"displayName":"Joseph Amess","userId":"01893786635442142975"}},"outputId":"007d2d3b-5dad-4d69-a2e0-7df5b21d78bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["... num tests to complete : 81 ... \n","\n","tests complete : 12.35% : best acc : 77.8%\n","tests complete : 24.69% : best acc : 78.0%\n","tests complete : 37.04% : best acc : 78.0%\n","tests complete : 49.38% : best acc : 79.2%\n","tests complete : 61.73% : best acc : 79.5%\n","tests complete : 74.07% : best acc : 79.9%\n","tests complete : 86.42% : best acc : 79.9%\n","tests complete : 98.77% : best acc : 81.2%\n","\n","... testing complete ... \n","\n","Best tree parameters:\n","\n","{'bag_size': 1000, 'num_trees': 50, 'num_features': 7, 'max_tree_depth': 12, 'num_partitions': 3, 'accuracy': 81.2}\n","accuracy of best forest on test set : 81.45%\n"]}],"source":["def main():\n","\n","  # make data\n","  data = dt.make_classification(\n","                                  n_samples = 20000,\n","                                  n_features = 10,\n","                                  n_informative = 8,\n","                                  n_redundant = 2,\n","                                  n_repeated = 0,\n","                                  n_classes = 2,\n","                                  n_clusters_per_class = 3\n","                              )\n","  splitVals = (0.7, 0.2, 0.1)\n","  split_data = Utilities.train_test_val_split(splitVals, data)\n","\n","  # tune hyperparameters\n","  param_grid = {  'bag_size'          : [500, 750, 1000],\n","                  'num_trees'         : [30, 40, 50],\n","                  'num_features'      : [5, 6, 7],\n","                  'max_tree_depth'    : [10, 12, 14],\n","                  'num_partitions'    : [3]\n","              }\n","\n","  total_tests = Utilities.grid_size_calc(param_grid)\n","  print(f'... num tests to complete : {total_tests} ... ')\n","  print('')\n","\n","  performance_dict = {}\n","  test_count = 0\n","  best_accuracy = 0\n","  for bag_size in param_grid['bag_size']:\n","      for num_trees in param_grid['num_trees']:\n","          for num_features in param_grid['num_features']:\n","              for max_tree_depth in param_grid['max_tree_depth']:\n","                  for num_partitions in param_grid['num_partitions']:\n","                      performance_dict[f'test_{test_count}'] = {  'bag_size'          : bag_size,\n","                                                                  'num_trees'         : num_trees,\n","                                                                  'num_features'      : num_features,\n","                                                                  'max_tree_depth'    : max_tree_depth,\n","                                                                  'num_partitions'    : num_partitions\n","                                                                  }\n","                      num_bags = num_trees\n","                      bagged_data = Utilities.bagger(bag_size, num_bags, num_features, split_data)\n","                      rfc = RandomForest( num_trees=num_trees, \n","                                          max_tree_depth=max_tree_depth, \n","                                          min_entropy=0, \n","                                          num_partitions=num_partitions)\n","                      forest = rfc.build_forest()\n","                      forest = rfc.fit(bagged_data=bagged_data, forest=forest)\n","                      val_predictions = rfc.predict(split_data[2], forest)\n","                      val_accuracy = Utilities.accuracy(split_data[2][1], val_predictions)\n","                      performance_dict[f'test_{test_count}']['accuracy'] = val_accuracy\n","                      if val_accuracy >= best_accuracy:\n","                          best_accuracy = val_accuracy\n","                          best_classifier = rfc\n","                          best_forest = forest\n","                          best_test = test_count\n","                      test_count += 1\n","                      if test_count % 10 == 0:\n","                          print(f'tests complete : {round(100 * (test_count / total_tests), 2)}% : best acc : {best_accuracy}%')\n","  print('')\n","  print('... testing complete ... ')\n","  print('')\n","  print('Best tree parameters:')\n","  print('')\n","  print(performance_dict[f'test_{best_test}'])\n","\n","  # performance on test set\n","  test_predictions = best_classifier.predict(split_data[1], best_forest)\n","  test_accuracy = Utilities.accuracy(split_data[1][1], test_predictions)\n","  print(f'accuracy of best forest on test set : {round(test_accuracy, 2)}%')\n","\n","\n","if __name__ == '__main__':\n","  main()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}
